epochs: 200
optimizers: #benchmarking optimizers
  SMB:
    lr: 0.5
    eta: 0.99
    c: 0.1
    independent_batch:
      - true
      - false
    autoschedule: false
    gamma: 0.05
    beta: 0.9
    lr_max: 20
  SLS:
    c: 0.1
    reset_option: 1
    init_step_size: 1
  ADAM:
    lr: 0.001
  SGD:
    lr: 0.1
preconfigs: #dataset and models
  - - mnist
    - mlp
  - - cifar10
    - resnet34_10
  - - cifar10
    - densenet10
  - - cifar100
    - resnet34_100
  - - cifar100
    - densenet100
  - - cifar10
    - vgg16
  - - cifar10
    - mobilenet
  - - cifar10
    - dla
  - - cifar10
    - dpn92
seed: 42
#n_train_samples:  #size of the subset of the dataset to load for training, comment to include all
batch_size: 128
